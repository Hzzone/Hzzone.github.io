[CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)

范数与距离的关系及应用

在 cs231n 中只是粗略一讲，未深入，这里将深入了解机器学习中的范数、距离衡量以及应用，例如 **范数规则化** 等等，在面试中经常被问到的问题。

### 参考资料
* [机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995/)
* [范数与距离的关系以及在机器学习中的应用](https://blog.csdn.net/kingzone_2008/article/details/15073987)
* [相似性度量：机器学习距离公式总结](https://www.cnblogs.com/lilin22/p/8747782.html)

### cs231n 距离介绍
* L1 距离（曼哈顿距离）:

$$d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$$

* L2 距离（欧式距离）:

$$d_2(I_1,I_2)=\sqrt{\sum_{p}\left(I^p_1-I^p_2\right)^2}$$

L1 和 L2 距离在坐标轴上具体表现为：

<div align=center>
   <img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-10-15-Screen%20Shot%202018-10-15%20at%206.28.43%20PM.png" />
</div>

L1 边上的点距离相等，而 L2 圆上的点距离相等，因此 L1 取决于选择的坐标系统，如果转动坐标轴会改变 L1 距离。而改变坐标轴对 L2 距离毫无影响，无论怎么转动都是不会变的。

所以 L1 有坐标依赖，实际依赖于数据的坐标系统，如果特征向量中每个元素有特别的意义，例如代表工作时间、年薪等，L1 会更好。如果只是某个空间中的通用向量，我们不知道里面实际代表的含义，则选择 L2。

实际用的时候两种都尝试一下，选择最好的。


L1 和 L2 在 KNN 上的差异：
<div align=center>
   <img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-10-15-Screen%20Shot%202018-10-15%20at%206.30.30%20PM.png" />
</div>

L1 边界趋向于跟随坐标轴，L2 边界不受坐标轴影响，放在了最自然的地方。

具体变现在 L1 的边界会非常值，趋向于接近坐标轴，L2 边界就看起来更自然，不像 L1 这么 **锋利**。

可以在 [K-Nearest Neighbors Demo](http://vision.stanford.edu/teaching/cs231n-demos/knn/) 切换 Metric 查看效果。

上图效果可能不明显，来一张更明显的：
<div align=center>
   <img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-10-15-download1%202.png" />
</div>

### 距离
首先假设两个 n 维向量，$A(x_{11},x_{12},...x_{1n})$ 与 $B(x_{21},x_{22},...x_{2n})$

* #### 闵可夫斯基距离（Minkowski Distance)

$$d_{12}=\sqrt[p]{\sum_{k=1}^{n}(x_{1k}-x_{2k})^{p}}$$

严格意义上来看，闵可夫斯基距离不是一种距离，而是一组距离的定义。
其中 $P$ 是一个变参数，当 $P=1$ 时，就是曼哈顿距离；当 $P=2$ 时，就是欧式距离；当 $P\to +\infty$ 时，就是切比雪夫距离。

因此我们根据Ｐ参数的不同，闵可夫斯基距离可以表示一类距离。

* #### 欧式距离（Euclidean Distance)

欧式距离即L2范数，是欧式空间两点间的距离公式。

$$d_{12}=\sqrt[2]{\sum_{k=1}^{n}(x_{1k}-x_{2k})^{2}}$$

具体到矩阵或者向量的求法为：

$$d_{12}=\sqrt{(A-B)(A-B)^T}$$

* #### 马氏距离（Mahalanobis Distance)
马氏距离是在欧式距离上发展而来，有 M 个样本向量 $x_{1}..x_{m}$，协方差矩阵为 $S$，均值为向量 $\mu$，向量 $x$ 到 $\mu$ 的马氏距离为：

$$D(x)=\sqrt{(x-\mu)^TS^{-1}(x-\mu)}$$
