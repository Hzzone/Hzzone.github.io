<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> </title>
    <link rel="shortcut icon" href="https://tuchuang-1252747889.cos.ap-guangzhou.myqcloud.com/hzzoneio_favicon.ico" />
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <link rel="stylesheet" href="../static/css/github-markdown.css" type="text/css">
    <link rel="stylesheet" href="../static/css/site.css" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/xcode.min.css">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      jax: ["input/TeX", "output/CommonHTML"]
    },
    CommonHTML: { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    SVG: { linebreaks: { automatic: true } }
  });
</script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
</head>
<body for="html-export">
    <div class="mume markdown-preview markdown-body">
        <div class="homepage">
            <a href="/">主页</a>
        </div>
            <br>

<p>notebook 地址: <a href="https://nbviewer.jupyter.org/github/Hzzone/hzzone.github.io/blob/source/implementation/%E7%90%86%E8%A7%A3%20PyTorch%20%E8%AE%A1%E7%AE%97%E5%9B%BE%E3%80%81Autograd%20%E6%9C%BA%E5%88%B6%E5%92%8C%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E6%8B%9F%E5%90%88.ipynb">理解 PyTorch 计算图、Autograd 机制和实现线性拟合.ipynb</a></p>
<h2 id=理解-pytorch-计算图-autograd-机制和实现线性拟合>理解 PyTorch 计算图、Autograd 机制和实现线性拟合</h2>
<h3 id=参考>参考</h3>
<ul>
<li><a href="http://www.cnblogs.com/armysheng/p/3422923.html">机器学习经典算法之-----最小二乘法</a></li>
<li><a href="https://pytorch.org/docs/stable/notes/autograd.html">Autograd mechanics</a></li>
<li><a href="https://blog.csdn.net/u013527419/article/details/70184690">计算图（computational graph）角度看BP（back propagation）算法</a></li>
<li><a href="https://blog.csdn.net/manong_wxd/article/details/78734358">PyTorch学习总结(七)——自动求导机制</a></li>
<li><a href="https://pytorch-cn.readthedocs.io/zh/latest/notes/autograd/">自动求导机制</a></li>
</ul>
<h3 id=计算图>计算图</h3>
<p>启发于 cs231n 的 <a href="http://cs231n.stanford.edu/syllabus.html">Deep Learning Hardware and Software</a>，课程在里面介绍了 pytorch 的自动求导机制和动态计算图。</p>
<p>我们都知道梯度下降和反向传播需要求解每一层的梯度，以更新权重。<a href="https://github.com/BVLC/caffe/tree/master/src/caffe/layers">Caffe Layers</a> 采用的机制是对每一层都定义一个 <code>backward</code> 和 <code>forward</code> 操作，然后在这两个函数中前馈、计算梯度等。</p>
<p>Computational Graph 指的是一系列的操作，包括输出的数据。</p>
<p>例如课程中的例子，利用 PyTorch 自动求导机制求 x, y 的梯度和 numpy 对比，中间则是计算图，实现的是 $c=\sum{(x*y+z)}$。</p>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-11-25-123659.png" alt=""></p>
<p>TensorFlow 的数据流图的例子，数据经过节点被处理，然后输出，就像水一样流动:</p>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-11-25-tensors_flowing.jpg" alt=""></p>
<p><strong>Pytorch 是动态图，每一次训练，都会销毁图并重新创建，这样做花销很大，但是更加灵活。</strong>而 Tensorflow 是静态图，一旦定义训练时就不能修改。Pytorch 合并 caffe2 发布 1.0 版本之后引入静态图，而 Tensorflow 已经发布 <a href="https://www.tensorflow.org/guide/eager">Eager Execution</a> 引入动态图。但相对来说还是 Pytorch 更加灵活。</p>
<h3 id=autograd-mechanics>Autograd mechanics</h3>
<p>首先要知道，Pytorch 的数据和参数（权重、偏置等）都是以 Tensor 存储的，Pytorch 的 Tensor 相当于 caffe 的 Blob，Tensorflow 的 Tensor。Pytorch 已经定义了 Tensor 的很多操作，可以在 GPU 上运算加速。</p>
<p>Pytorch 会自动跟踪计算图的操作，在计算图执行完成后，调用 <code>backward</code> 计算梯度。很久以前数据需要使用 <code>Variable</code> 封装，像这样的:</p>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-11-25-68960-7084a4be66464e40.png" alt=""></p>
<p>data 存储数据，grad 存储梯度，creator 指向创建者。</p>
<p>现在不用这么麻烦了，直接合并 Tensor 和 Variable，只需要 <code>requires_grad=True</code> 表明需要计算梯度。</p>
<p>需要注意: <strong>只有当任意一个输入的 Tensor 不需要计算梯度时，输出才不需要计算梯度；如果有一个需要计算，输出就需要。</strong></p>
<pre><code class="python">import torch
x = torch.randn(5, 5)
y = torch.randn(5, 5)
z = x + y
z.requires_grad</code></pre>
<pre><code>False</code></pre><pre><code class="python">x = torch.randn(5, 5, requires_grad=True)
y = torch.randn(5, 5)
z = x + y
z.requires_grad</code></pre>
<pre><code>True</code></pre><p>实现自定义的 Tensor 自动求导需要实现 <code>forward</code> 和 <code>backward</code>，例如 cs231n 的例子:</p>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-11-25-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-25%20%E4%B8%8B%E5%8D%889.10.56.png" alt=""></p>
<p><code>backward</code> 之后更新完参数需要清除梯度，<code>torch.no_grad</code> 是指在此的计算不创建图的节点。</p>
<h3 id=实现线性拟合>实现线性拟合</h3>
<p>线性拟合是初中的知识，很简单，就是用一条直线拟合一些点，并使得点到直线的距离之和最短。常用最小二乘法求解，这里使用梯度下降（虽然线性拟合有公式直接求解）。点到直线的距离公式:</p>
<p>$$
d = \left| \frac { A x _ { 0 } + B y _ { 0 } + C } { \sqrt { A ^ { 2 } + B ^ { 2 } } } \right|
$$</p>
<p>接下来我就生成一些点，并假设直线为 $y=ax+b$，那么点到直线的距离为:</p>
<p>$$
d = \left| \frac { a x _ { 0 } - y _ { 0 } + b } { \sqrt { a ^ { 2 } + 1 } } \right|
$$</p>
<p>因此定义损失函数为:</p>
<p>$$
L = \frac{1}{2N}\sum_i \frac{(a x _ { i } - y _ { i } + b)^2}{a^2+1}
$$</p>
<p>显然可以对 $L$ 直接求导，但是这个公式直接对 $a$ 和 $b$ 求偏导太麻烦，最后的结果很复杂，可能这就是距离公式常用 MSE 和 MAE 的原因。但是我可以用 Pytorch 的计算图自动求导解决。</p>
<p>生成一些随机点:</p>
<pre><code class="python">import numpy as np
import random
import matplotlib.pyplot as plt
# 在0-2*pi的区间上生成100个点作为输入数据
X = np.linspace(0, 100, 100,endpoint=True)
a, b = 2.5, 1.0
Y = a*X+b

# 对输入数据加入gauss噪声
# 定义gauss噪声的均值和方差
mu = 0
sigma = 2
Nx, Ny = X.copy(), Y.copy()
for i in range(X.shape[0]):
    Nx[i] += random.gauss(mu,sigma)
    Ny[i] += random.gauss(mu,sigma)

# 画出这些点
plt.plot(X, Y)
plt.scatter(Nx, Ny, marker=&#39;.&#39;, color=&#39;r&#39;)
plt.show()</code></pre>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-11-25-output_32_0.png" alt=""></p>
<p>接下来就是一系列计算然后更新 $a$ 和 $b$ 了。</p>
<pre><code class="python">X = torch.from_numpy(Nx)
Y = torch.from_numpy(Ny)
X = X.float()
Y = Y.float()
lr = 1e-9
iA, iB = torch.Tensor([0]), torch.Tensor([0])
# 需要计算梯度
iA.requires_grad = True
iB.requires_grad = True

# 记录最好的结果
best_loss = float(&quot;inf&quot;)
best_a = 0.0
best_b = 0.0
loss_list = []
max_epochs = 1000

# 梯度下降
for _ in range(max_epochs):
    # 计算 loss
    loss = torch.mean((iA*X-Y+iB)**2/(iA**2+1))
    # 反向传播
    loss.backward()
    # 更新参数
    with torch.no_grad():
        iA -= lr*iA.grad
        iB -= lr*iB.grad
        cur_loss = loss.item()
        if cur_loss&lt;best_loss:
            best_a = iA.item()
            best_b = iB.item()
            best_loss = cur_loss
        loss_list.append(cur_loss)

# loss 曲线
plt.plot(list(range(max_epochs)), loss_list)
plt.show()</code></pre>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-11-25-output_34_0.png" alt=""></p>
<pre><code class="python">best_a, best_b, best_loss</code></pre>
<pre><code>(2.511101245880127, 0.02796928584575653, 4.386769771575928)</code></pre><pre><code class="python">nX = np.linspace(0, 100, 100,endpoint=True)
plt.plot(nX, best_a*nX+best_b)
plt.scatter(Nx, Ny, marker=&#39;.&#39;, color=&#39;r&#39;)
plt.show()</code></pre>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-11-25-output_36_0.png" alt=""></p>
<p>最后可以看到拟合效果已经很好了。调节 learning rate 可以获得更好的效果，loss 曲线最后又增大是因为可能在最小值点，loss 震荡，无法到达最小值点。这点可以参考我的梯度下降的理解博客。</p>
<p>当 learning rate 较大时，拟合的效果没有这么好。<strong>总结就是梯度下降法应用 learning rate deacy 的非常重要。</strong></p>

        <script data-isso="https://hzzone.io/isso"
        src="https://hzzone.io/isso/js/embed.min.js"></script>

        <section id="isso-thread" style="padding-bottom: 20px;"></section>
    </div>
<script>
(function(e,t,n,i,s,a,c){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)}
;a=t.createElement(i);c=t.getElementsByTagName(i)[0];a.async=true;a.src=s
;c.parentNode.insertBefore(a,c)
})(window,document,"galite","script","https://cdn.jsdelivr.net/npm/ga-lite@2/dist/ga-lite.min.js");

galite('create', 'UA-128984734-1', 'auto');
galite('send', 'pageview');
</script>
</body>
</html>