<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="UTF-8">
        <title>理解 Weight Regularization 和及其实现</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="shortcut icon" href="https://tuchuang-1252747889.cos.ap-guangzhou.myqcloud.com/hzzoneio_favicon.ico" />
        <!--<link rel="stylesheet" type="text/css" href="css/aircloud.css">-->
        <link rel="stylesheet" type="text/css" href="/static/css/aircloud.css">
        <link rel="stylesheet" type="text/css" href="/static/css/next.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/3.0.1/github-markdown.min.css" />
        <link rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/styles/default.min.css">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/xcode.min.css">
        <script type="text/javascript" defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script async src="https:////busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <link rel="stylesheet" href="/static/css/iconfont.css">
        <script src="/static/js/iconfont.js"></script>
        <!--<link href="https://cdn.bootcss.com/font-awesome/5.8.2/css/fontawesome.min.css" rel="stylesheet">-->
        <!--<link href="https://cdn.bootcss.com/font-awesome/5.8.2/css/all.min.css" rel="stylesheet">-->
    </head>

    <body>
        <div id="progress-bar"></div>
        <div class="site-nav-toggle" id="site-nav-toggle">
            <button>
                <span class="btn-bar"></span>
                <span class="btn-bar"></span>
                <span class="btn-bar"></span>
            </button>
        </div>
        <div class="index-about">
            <i>To be talented & positive.</i>
        </div>
        <div class="index-container">
            <div class="index-left">
                <div class="nav" id="nav">
                    <div class="avatar-name">
                        <div class="avatar">
                            <img src="https://avatars2.githubusercontent.com/u/19267349"></div>
                        <div class="name">
                            <i>Zhizhong Huang</i>
                        </div>
                    </div>
                    <div class="contents" id="nav-content">
                        <ul>
                            <li style="padding-left: 10px;">
                                <a href="/index.html">
                                    <!--<i class="fa fa-fw fa-home"></i>-->
                                    <!--<span class="iconfont icon-zhihu"></span>-->
                                    <!--<i class="iconfont icon-home"></i>-->
                                    <span>主页</span>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/Hzzone">
                                    <i class="iconfont icon-github"></i>
                                </a>
                                <a href="https://www.zhihu.com/people/hzzone">
                                    <i class="iconfont icon-zhihu"></i>
                                </a>
                            </li>
                            <!--<li>-->
                                <!--<a href="https://github.com/Hzzone">-->
                                    <!--&lt;!&ndash;<i class="fab fa-github"></i>&ndash;&gt;-->
                                    <!--<i class="iconfont icon-github"></i>-->
                                    <!--<span>Github</span></a>-->
                            <!--</li>-->
                            <!--<li>-->
                                <!--<a href="https://www.zhihu.com/people/hzzone">-->
                                    <!--&lt;!&ndash;<i class="fa fa-fw fa-zhihu"></i>&ndash;&gt;-->
                                    <!--<i class="iconfont icon-zhihu"></i>-->
                                    <!--<span>知乎</span>-->
                                <!--</a>-->
                            <!--</li>-->
                        </ul>
                    </div>
                    <!--<div id="toc" class="toc-article toc-fixed" style="height: 823px; overflow-y: scroll;">-->
                        <!--<ol class="toc">-->
                            <!--<li class="toc-item toc-level-3 active">-->
                                <!--<a class="toc-link" href="#目前遇到的问题">-->
                                    <!--<span class="toc-text">目前遇到的问题</span></a>-->
                            <!--</li>-->
                            <!--<li class="toc-item toc-level-3">-->
                                <!--<a class="toc-link" href="#问题归纳与解决">-->
                                    <!--<span class="toc-text">问题归纳与解决</span></a>-->
                            <!--</li>-->
                            <!--<li class="toc-item toc-level-3">-->
                                <!--<a class="toc-link" href="#更多">-->
                                    <!--<span class="toc-text">更多</span></a>-->
                            <!--</li>-->
                        <!--</ol>-->
                    <!--</div>-->
                </div>
                <div class="index-about-mobile">
                    <i>To be talented & positive.</i>
                </div>
            </div>
            <div class="content-wrap">
                <!-- Main Content -->
                <div class="post-container">
  <div class="post-title">理解 Weight Regularization 和及其实现</div>
  <div class="post-meta">
    <span class="attr">发布于：
      <span>2018-12-05 09:31:05</span>
    </span>
    <span class="attr">标签：
      <span>cs231n</span>
      </span>
    <span class="attr">访问：
      <span id="busuanzi_value_page_pv">1368</span></span>
  </div>
    <div class="markdown-body post-content post_href">
        
        <h3>参考</h3>
<ul>
<li><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s5ss1.html">正则化</a></li>
</ul>
<h3>Weight Regularization（正则化）</h3>
<p>Weight Regularization 在 cs231n 的 <a href="http://cs231n.stanford.edu/syllabus.html">Loss Functions and Optimization </a> 提及。我觉得这篇文章 <a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s5ss1.html">正则化</a> 写得很详细，可以参考一下，不过其中应该有个错误正则项应该是不需要除 $n$ 的。</p>
<p>下面是 cs231n 涉及到正则化的内容，非常直观，我就不再多写了:</p>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-12-02-%E6%9C%AA%E5%91%BD%E5%90%8D%E6%8B%BC%E5%9B%BE%20-1-.jpg" alt=""></p>
<p>首先 Weight Regularization 是解决过拟合的一种方法，提高模型泛化能力，其他的还有 Dropout、Batch Norm 等。</p>
<p>Weight Regularization 起作用主要是约束模型复杂度，获得更简单的权重。</p>
<ul>
<li>cs231n 的例子</li>
</ul>
<p>假设 $W$ 是最优解，$2W$ 的结果也一样。上图中 $w _ { 1 } ^ { T } x = w _ { 2 } ^ { T } x = 1$，$w_2$ 的 Frobenius 范数更小，所以 $w_2$ 更简单。区别在于 L2 范数将权重跟趋向于均匀分布（展开），而不是极端分布。</p>
<ul>
<li><a href="http://lamda.nju.edu.cn/weixs/book/CNN_book.html">解析深度学习——卷积神经网络原理与视觉实践</a></li>
</ul>
<blockquote>
<p>如图，如果将模型原始的假设空间比做“天空”， 那么天空中自由飞翔的“鸟”就是模型可能收敛到的一个个最优解。 在施加了模型正则化后，就好比将原假设空间（“天空”）缩小到一定的空间范围（“笼子”），这样一来，可能得到的最优解（“鸟”）能搜寻的假设空间也变得相对有限。有限空间自然对应复杂度不太高的模型，也自然对应了有限的模型表达能力，这就是“正则化能有效防止模型过拟合”的一种直观解释。许多浅层学习器（如支持向量机等）为了提高泛化性往往都要依赖模型正则 化，深度学习更应如此。深度网络模型相比浅层学习器巨大的多的模型复杂度 是把更锋利的双刃剑：保证模型更强大表示能力的同时也使模型蕴藏着更巨大的过拟合风险。深度模型的正则化可以说是整个深度模型搭建的最后一步，更是不可缺少的重要一步。</p>
</blockquote>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-12-03-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-03%20%E4%B8%8B%E5%8D%889.50.54.png" alt=""></p>
<ul>
<li>Deep Learning 书上的例子</li>
</ul>
<p>为什么不对偏置正则化:</p>
<blockquote>
<p>在探究不同范数的正则化表现之前，我们需要说明一下，在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常只对权重做惩罚而不对偏置做正则惩罚。 精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。因此，我们使用向量 w 表示所有应受范 数惩罚影响的权重，而向量 θ 表示所有参数 (包括 w 和无需正则化的参数)。在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的 α 系数。寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。</p>
</blockquote>
<p>书中在 <strong>第七章 深度学习中的正则化</strong> 对正则化的证明、作用有很深的探讨，比上面的两个介绍的多。而且证明的很漂亮，强烈推荐。</p>
<h4>L2 正则化</h4>
<p>$L^2$ 参数范数惩罚又被叫做权重衰减（weight decay），在其他学术圈，也被称为岭回归或 Tikhonov 正则。</p>
<p>$L^2$ 正则化的公式为:</p>
<p>$$
\tilde { J } (  w  ;  X  ,  y  ) = \frac { \alpha } { 2 } \| \boldsymbol { w } \| _ { 2 } ^ { 2 }+ J (  w  ;  X ,  y  )
$$</p>
<p>$\alpha$ 控制正则项大小，较大的 $\alpha$ 取值将较大程度约束模型复杂度；反之易然。</p>
<p>在原来的损失函数基础上加上了 $\Omega ( \boldsymbol { \theta } ) = \frac { 1 } { 2 } \| \boldsymbol { w } \| _ { 2 } ^ { 2 }$ 的正则项。</p>
<p>对 $w$ 进行求导:</p>
<p>$$
\nabla _ { w } \tilde { J } ( w  ;  X  , y  ) = \alpha  w  + \nabla _ { w } J (  w  ;  X  ,  y  )
$$</p>
<p>更新权重时的公式变为:</p>
<p>$$
\boldsymbol { w } \leftarrow  w  - \epsilon \left( \alpha w  + \nabla _ { w } J (  w  ; X  ,y  ) \right)
$$</p>
<p>$$
w  \leftarrow ( 1 - \epsilon \alpha )  w  - \epsilon \nabla _ { w } J (  w  ; X  ,  y  )
$$</p>
<p>在每步执行通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因子）。</p>
<p>再贴一些来自 Deep Learning 书上的说明:</p>
<blockquote>
<p>只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函 数减小的方向（对应 Hessian 矩阵较小的特征值）上改变参数不会显著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。</p>
</blockquote>
<blockquote>
<p>我们可以看到，L2 正则化能让学习算法 ‘‘感知’’ 到具有较高方差的输入 x，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。</p>
</blockquote>
<p>有些地方我也没看懂，难过。</p>
<h3>L1 正则化</h3>
<p>L1 正则项为:</p>
<p>$$
\Omega (  \theta  ) = \|  w  \| _ { 1 } = \sum _ { i } \left| w _ { i } \right|
$$</p>
<p>整体代价函数:</p>
<p>$$
\tilde { J } (  w  ;  X  ,  y  ) = \alpha \|  w  \| _ { 1 } + J (  w  ;  X  ,  y  )
$$</p>
<p>对应的梯度为:</p>
<p>$$
\nabla _ { w } \tilde { J } (  w  ;  X  ,  y  ) = \alpha \operatorname { sign } (  w  ) + \nabla _ { w } J (  w  ;  X  ,  y  )
$$</p>
<p>sign 函数大于 0 为 1，小于 0 为 -1，等于 0 为 0。</p>
<p>L1 与 L2 的不同之处:</p>
<blockquote>
<p>我们立刻发现 L1 的正则化效果与 L2 大不一样。具体来说，我们可以看到正则化对梯度的影响不再是线性地缩放每个 $w_i$；而是添加了一项与 $sign(w_i)$ 同号的常数。</p>
</blockquote>
<blockquote>
<p>相比 L2 正则化，L1 正则化会产生更稀疏（sparse）的解。此处稀疏性指的是最优值中的一些参数为 0。</p>
</blockquote>
<blockquote>
<p>稀疏化的结果使优化后的参数一部分为 0，另一部分为非零实值。非零实值的那部分参数可起到选择重要参数或特征维度的作用，同时可起到去除噪声的效果。</p>
</blockquote>
<h4>Elastic 正则化</h4>
<p>联合使用 L1、L2 正则化，正则项为:</p>
<p>$$
\alpha _ { 1 } \| \omega \| _ { 1 } + \alpha _ { 2 } \| \alpha \| _ { 2 } ^ { 2 }
$$</p>
<h3>正则化实现</h3>
<p>我前面实现了线性层、交叉熵等，我直接复制不需要改的代码，然后修改线性层实现包括 L1、L2 正则化。</p>
<p>再对比一下三个的效果。</p>
<p><strong>复制不需要改的代码，保存在最后一个单元格。</strong></p>
<p>原先的实现没有正则化，在这里直接修改，关于梯度在上面已经求了。</p>
<pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">l1_regularization</span><span class="hljs-params">(W, alpha)</span>:</span>
    <span class="hljs-keyword">return</span> alpha*np.sign(W)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">l2_regularization</span><span class="hljs-params">(W, alpha)</span>:</span>
    <span class="hljs-keyword">return</span> alpha*W

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">no_regularization</span><span class="hljs-params">(W, alpha)</span>:</span>
    <span class="hljs-keyword">return</span> np.zeros_like(W)

regularize = {
    <span class="hljs-number">0</span>: no_regularization,
    <span class="hljs-number">1</span>: l1_regularization,
    <span class="hljs-number">2</span>: l2_regularization
}
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, D_in, D_out, regularization=<span class="hljs-number">0</span>, alpha=<span class="hljs-number">0</span>)</span>:</span>
        self.weight = np.random.randn(D_in, D_out).astype(np.float32)*<span class="hljs-number">0.01</span>
        self.bias = np.zeros((<span class="hljs-number">1</span>, D_out), dtype=np.float32)
        self.regularization = regularization
        self.alpha = alpha
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input)</span>:</span>
        self.data = input
        <span class="hljs-keyword">return</span> np.dot(self.data, self.weight)+self.bias
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, top_grad, lr)</span>:</span>
        self.grad = np.dot(top_grad, self.weight.T).astype(np.float32)
        grad_w = np.dot(self.data.T, top_grad)
        <span class="hljs-comment"># 加上正则项求导</span>
        grad_w += regularize[self.regularization](self.weight, self.alpha)
        <span class="hljs-comment"># 更新参数</span>
        self.weight -= lr*grad_w
        self.bias -= lr*np.mean(top_grad, axis=<span class="hljs-number">0</span>)
</code></pre>
<p>更新了一下线性层如果加上正则化项之后的反向传播的关于权重 $w$ 的梯度，bias 没有正则项。</p>
<p>我在写一个通用的训练函数，测试一下这三种情况的结果有什么区别。</p>
<pre class="hljs"><code><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm_notebook
<span class="hljs-keyword">import</span> copy

batch_size = <span class="hljs-number">120</span>
<span class="hljs-comment"># 读取并归一化数据，不归一化会导致 nan</span>
test_data = ((read_mnist(<span class="hljs-string">'../data/mnist/t10k-images.idx3-ubyte'</span>).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">784</span>))<span class="hljs-number">-127.0</span>)/<span class="hljs-number">255.0</span>).astype(np.float32)
train_data = ((read_mnist(<span class="hljs-string">'../data/mnist/train-images.idx3-ubyte'</span>).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">784</span>))<span class="hljs-number">-127.0</span>)/<span class="hljs-number">255.0</span>).astype(np.float32)
<span class="hljs-comment"># 独热编码标签</span>
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder
encoder = OneHotEncoder()
encoder.fit(np.arange(<span class="hljs-number">10</span>).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)))
train_labels = encoder.transform(read_mnist(<span class="hljs-string">'../data/mnist/train-labels.idx1-ubyte'</span>).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>))).toarray().astype(np.float32)
test_labels = encoder.transform(read_mnist(<span class="hljs-string">'../data/mnist/t10k-labels.idx1-ubyte'</span>).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>))).toarray().astype(np.float32)
train_dataloader = Dataloader(train_data, train_labels, batch_size, shuffle=<span class="hljs-literal">True</span>)
test_dataloader = Dataloader(test_data, test_labels, batch_size, shuffle=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># net 应该是一个一层的线性网络</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_and_test</span><span class="hljs-params">(loss_layer, net, scheduler, max_iter, train_dataloader, test_dataloader)</span>:</span>
    test_loss_list, train_loss_list, train_acc_list, test_acc_list = [], [], [], []
    best_net = <span class="hljs-literal">None</span>
    <span class="hljs-comment"># 最高准确度，和对应权重</span>
    best_acc = -float(<span class="hljs-string">'inf'</span>)
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> tqdm_notebook(range(max_iter)):
        <span class="hljs-comment"># 训练</span>
        correct = <span class="hljs-number">0</span>
        total_loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> data, labels <span class="hljs-keyword">in</span> train_dataloader:
            <span class="hljs-comment"># 前向输出概率</span>
            train_pred = net.forward(data)

            <span class="hljs-comment"># 计算准确度</span>
            pred_labels = np.argmax(train_pred, axis=<span class="hljs-number">1</span>)
            real_labels = np.argmax(labels, axis=<span class="hljs-number">1</span>)
            correct += np.sum(pred_labels==real_labels)

            <span class="hljs-comment"># 前向输出损失</span>
            loss = loss_layer.forward(train_pred, labels)
            total_loss += loss*data.shape[<span class="hljs-number">0</span>]
            

            <span class="hljs-comment"># 反向更新参数</span>
            loss_layer.backward()
            net.backward(loss_layer.grad, scheduler.get_lr())
            
        total_loss /= len(train_dataloader)
        <span class="hljs-keyword">if</span> net.regularization==<span class="hljs-number">0</span>:
            reg_loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">elif</span> net.regularization==<span class="hljs-number">1</span>:
            reg_loss = np.sum(net.weight)*net.alpha
        <span class="hljs-keyword">else</span>:
            reg_loss = np.sqrt(np.sum(np.square(net.weight)))*net.alpha/<span class="hljs-number">2</span>
        total_loss += reg_loss
        
        acc = correct/len(train_dataloader)
        train_acc_list.append(acc)
        train_loss_list.append(total_loss)
        scheduler.step()
        
        <span class="hljs-comment"># 测试</span>
        correct = <span class="hljs-number">0</span>
        total_loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> data, labels <span class="hljs-keyword">in</span> test_dataloader:
            <span class="hljs-comment"># 前向输出概率</span>
            test_pred = net.forward(data)

            <span class="hljs-comment"># 前向输出损失</span>
            loss = loss_layer.forward(test_pred, labels)
            total_loss += loss*data.shape[<span class="hljs-number">0</span>]

            <span class="hljs-comment"># 计算准确度</span>
            pred_labels = np.argmax(test_pred, axis=<span class="hljs-number">1</span>)
            real_labels = np.argmax(labels, axis=<span class="hljs-number">1</span>)
            correct += np.sum(pred_labels==real_labels)
            
        total_loss /= len(test_dataloader)
        
        <span class="hljs-comment"># 正则项损失因为没有更新参数所以不变</span>
        total_loss += reg_loss
        
        acc = correct/len(test_dataloader)
        test_acc_list.append(acc)
        test_loss_list.append(total_loss)

        <span class="hljs-keyword">if</span> acc &gt; best_acc: 
            best_acc = acc
            best_net = copy.deepcopy(net)
    <span class="hljs-keyword">return</span> test_loss_list, train_loss_list, train_acc_list, test_acc_list, best_net
</code></pre>
<p>初始化各项参数:</p>
<pre class="hljs"><code><span class="hljs-comment"># 损失层</span>
loss_layer = CrossEntropyLossLayer()
<span class="hljs-comment"># 输入输出维度</span>
D, C = <span class="hljs-number">784</span>, <span class="hljs-number">10</span>
np.random.seed(<span class="hljs-number">1</span>) <span class="hljs-comment"># 固定随机生成的权重</span>
</code></pre>
<p>开始训练:</p>
<pre class="hljs"><code><span class="hljs-comment"># 最大迭代次数和步长</span>
max_iter = <span class="hljs-number">120</span>
step_size = <span class="hljs-number">50</span>
<span class="hljs-comment"># 学习率</span>
lr = <span class="hljs-number">0.1</span>
<span class="hljs-comment"># 学习率衰减</span>
scheduler = lr_scheduler(lr, step_size)
linear_classifer_0 = Linear(D, C)
test_loss_list0, train_loss_list0, train_acc_list0, test_acc_list0, best_net0 = train_and_test(loss_layer, linear_classifer_0, scheduler, max_iter, train_dataloader, test_dataloader)
</code></pre>
<pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show</span><span class="hljs-params">(max_iter, train_loss_list, test_loss_list, train_acc_list, test_acc_list)</span>:</span>
    plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
    plt.title(<span class="hljs-string">'loss'</span>)
    plt.plot(range(max_iter), train_loss_list, label=<span class="hljs-string">'train_loss'</span>)
    plt.plot(range(max_iter), test_loss_list, label=<span class="hljs-string">'test_loss'</span>)
    plt.legend()
    plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
    plt.title(<span class="hljs-string">'accuracy'</span>)
    plt.plot(range(max_iter), train_acc_list, label=<span class="hljs-string">'train_acc'</span>)
    plt.plot(range(max_iter), test_acc_list, label=<span class="hljs-string">'test_acc'</span>)
    plt.legend()
    plt.subplots_adjust(hspace=<span class="hljs-number">0.5</span>)
    
show(max_iter, train_loss_list0, test_loss_list0, train_acc_list0, test_acc_list0)
</code></pre>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-12-05-output_50_0.png" alt=""></p>
<pre class="hljs"><code><span class="hljs-comment"># 最大迭代次数和步长</span>
max_iter = <span class="hljs-number">120</span>
step_size = <span class="hljs-number">50</span>
<span class="hljs-comment"># 学习率</span>
lr = <span class="hljs-number">0.1</span>
<span class="hljs-comment"># 学习率衰减</span>
scheduler = lr_scheduler(lr, step_size)
linear_classifer_1 = Linear(D, C, regularization=<span class="hljs-number">1</span>, alpha=<span class="hljs-number">1e-4</span>)
test_loss_list1, train_loss_list1, train_acc_list1, test_acc_list1, best_net1 = train_and_test(loss_layer, linear_classifer_1, scheduler, max_iter, train_dataloader, test_dataloader)
</code></pre>
<pre><code>HBox(children=(IntProgress(value=0, max=120), HTML(value='')))
</code></pre>
<pre class="hljs"><code>show(max_iter, train_loss_list1, test_loss_list1, train_acc_list1, test_acc_list1)
</code></pre>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-12-05-output_52_0.png" alt=""></p>
<pre class="hljs"><code><span class="hljs-comment"># 最大迭代次数和步长</span>
max_iter = <span class="hljs-number">120</span>
step_size = <span class="hljs-number">50</span>
<span class="hljs-comment"># 学习率</span>
lr = <span class="hljs-number">0.1</span>
<span class="hljs-comment"># 学习率衰减</span>
scheduler = lr_scheduler(lr, step_size)
linear_classifer_2 = Linear(D, C, regularization=<span class="hljs-number">2</span>, alpha=<span class="hljs-number">1e-3</span>)
test_loss_list2, train_loss_list2, train_acc_list2, test_acc_list2, best_net2 = train_and_test(loss_layer, linear_classifer_2, scheduler, max_iter, train_dataloader, test_dataloader)
</code></pre>
<pre><code>HBox(children=(IntProgress(value=0, max=120), HTML(value='')))
</code></pre>
<pre class="hljs"><code>show(max_iter, train_loss_list2, test_loss_list2, train_acc_list2, test_acc_list2)
</code></pre>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-12-05-output_54_0.png" alt=""></p>
<p>可视化无正则、L1 正则、L2 正则的权重差别:</p>
<pre class="hljs"><code><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

plt.hist(best_net0.weight.ravel(), bins=np.arange(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.01</span>), label=<span class="hljs-string">'no regularization'</span>)
plt.hist(best_net1.weight.ravel(), bins=np.arange(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.01</span>), label=<span class="hljs-string">'l1 regularization'</span>)
plt.hist(best_net2.weight.ravel(), bins=np.arange(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.01</span>), label=<span class="hljs-string">'l2 regularization'</span>)
plt.legend()
plt.show()
</code></pre>
<p><img src="https://tuchuang-1252747889.cosgz.myqcloud.com/2018-12-05-output_56_0.png" alt=""></p>
<p>在可视化一下三者之间的收敛速度:</p>
<p><strong>从以上的结果可以得出几个结论:</strong></p>
<ul>
<li>相对于无正则化之后的权重方差更小。</li>
<li>和 L2 相比，L1 使权重更稀疏，看他的 0 更突出。</li>
<li>同样的参数下 L2 的约束能力比 L1 要强，所以需要注意一下正则参数的大小不要太大，否则不能收敛（欠拟合）。</li>
<li>test accuracy 和 train accuracy 之间的 gap 变小（正则化的意义）。</li>
</ul>
<pre class="hljs"><code>np.std(best_net0.weight), np.std(best_net1.weight), np.std(best_net2.weight)
</code></pre>
<pre><code>(0.18500698, 0.14412738, 0.11477291)
</code></pre>
<p>测试集上的准确度也没有太大区别，一点小差距调整下步长和迭代次数就可以 work 了。</p>
<pre class="hljs"><code>np.max(test_acc_list0), np.max(test_acc_list1), np.max(test_acc_list2)
</code></pre>
<pre><code>(0.9257, 0.9232, 0.9218)
</code></pre>
<pre class="hljs"><code>
</code></pre>
<p><strong>最后总结一下，为了防止过拟合，减小 test accuracy 和 train accuracy 之间的 gap，非常需要权重衰减，获得更简单的权重。</strong></p>
<p><strong>一下是需要的代码</strong></p>
<pre class="hljs"><code><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> struct
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span><span class="hljs-params">(input)</span>:</span>
    exp_value = np.exp(input) <span class="hljs-comment">#首先计算指数</span>
    output = exp_value/np.sum(exp_value, axis=<span class="hljs-number">1</span>)[:, np.newaxis] <span class="hljs-comment"># 然后按行标准化</span>
    <span class="hljs-keyword">return</span> output

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CrossEntropyLossLayer</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">pass</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input, labels)</span>:</span>
        <span class="hljs-comment"># 做一些防止误用的措施，输入数据必须是二维的，且标签和数据必须维度一致</span>
        <span class="hljs-keyword">assert</span> len(input.shape)==<span class="hljs-number">2</span>, <span class="hljs-string">'输入的数据必须是一个二维矩阵'</span>
        <span class="hljs-keyword">assert</span> len(labels.shape)==<span class="hljs-number">2</span>, <span class="hljs-string">'输入的标签必须是独热编码'</span>
        <span class="hljs-keyword">assert</span> labels.shape==input.shape, <span class="hljs-string">'数据和标签数量必须一致'</span>
        self.data = input
        self.labels = labels
        self.prob = np.clip(softmax(input), <span class="hljs-number">1e-9</span>, <span class="hljs-number">1.0</span>) <span class="hljs-comment">#在取对数时不能为 0，所以用极小数代替 0</span>
        loss = -np.sum(np.multiply(self.labels, np.log(self.prob)))/self.labels.shape[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">return</span> loss
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self)</span>:</span>
        self.grad = (self.prob - self.labels)/self.labels.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># 根据公式计算梯度</span>

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Dataloader</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, data, labels, batch_size, shuffle=True)</span>:</span>
        self.data = data
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.labels = labels
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, index)</span>:</span>
        <span class="hljs-keyword">return</span> self.data[index], self.labels[index]
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__iter__</span><span class="hljs-params">(self)</span>:</span>
        datasize = self.data.shape[<span class="hljs-number">0</span>]
        data_seq = np.arange(datasize)
        <span class="hljs-keyword">if</span> self.shuffle:
            np.random.shuffle(data_seq)
        interval_list = np.append(np.arange(<span class="hljs-number">0</span>, datasize, self.batch_size), datasize)
        <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> range(interval_list.shape[<span class="hljs-number">0</span>]<span class="hljs-number">-1</span>):
            s = data_seq[interval_list[index]:interval_list[index+<span class="hljs-number">1</span>]]
            <span class="hljs-keyword">yield</span> self.data[s], self.labels[s]
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.data.shape[<span class="hljs-number">0</span>]
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">lr_scheduler</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, base_lr, step_size, deacy_factor=<span class="hljs-number">0.1</span>)</span>:</span>
        self.base_lr = base_lr <span class="hljs-comment"># 最初的学习率</span>
        self.deacy_factor = deacy_factor <span class="hljs-comment"># 学习率衰减因子</span>
        self.step_count = <span class="hljs-number">0</span> <span class="hljs-comment"># 当前的迭代次数</span>
        self.lr = base_lr <span class="hljs-comment"># 当前学习率</span>
        self.step_size = step_size <span class="hljs-comment"># 步长</span>
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span><span class="hljs-params">(self, step_count=<span class="hljs-number">1</span>)</span>:</span> <span class="hljs-comment"># 默认 1 次</span>
        self.step_count += step_count
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_lr</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment"># 根据公式 12 实现</span>
        self.lr = self.base_lr*(self.deacy_factor**(self.step_count//self.step_size)) <span class="hljs-comment"># 实现上面的公式</span>
        <span class="hljs-keyword">return</span> self.lr

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_mnist</span><span class="hljs-params">(filename)</span>:</span>
    <span class="hljs-keyword">with</span> open(filename, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
        zero, data_type, dims = struct.unpack(<span class="hljs-string">'&gt;HBB'</span>, f.read(<span class="hljs-number">4</span>))
        shape = tuple(struct.unpack(<span class="hljs-string">'&gt;I'</span>, f.read(<span class="hljs-number">4</span>))[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> range(dims))
        <span class="hljs-keyword">return</span> np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)
</code></pre>
<pre class="hljs"><code>
</code></pre>


  </div>
</div>

                <script data-isso="https://hzzone.me/isso"
                        src="https://hzzone.me/isso/js/embed.min.js"></script>

                <div class="comments">
                    <section id="isso-thread"></section>
                </div>
            </div>

        </div>
        <script type="text/javascript">const navToggle = document.getElementById('site-nav-toggle');
            navToggle.addEventListener('click',
            function() {
                let aboutContent = document.getElementById('nav-content');
                if (!aboutContent.classList.contains('show-block')) {
                    aboutContent.classList.add('show-block');
                    aboutContent.classList.remove('hide-block');
                } else {
                    aboutContent.classList.add('hide-block');
                    aboutContent.classList.remove('show-block');
                }
            });</script>
    <script type="text/javascript">
  window.MathJax = {
    jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    },
    CommonHTML: { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
  };
</script>
    <script>
        var domDiv = document.getElementById('progress-bar');
        //domH:可视区域的高度
        var domH = window.innerHeight || document.documentElement.clientHeight || document.body.clientHeight;
        window.addEventListener('scroll',function(){
            var pageHeight = Math.max(
             document.body.scrollHeight,
             document.documentElement.scrollHeight,
             document.body.offsetHeight,
             document.documentElement.offsetHeight,
             document.documentElement.clientHeight
            );
            domDiv.style.width = Math.round(pageYOffset/(pageHeight-domH)*100)+'%';
        },false);

        var posts = document.getElementsByClassName('post_href')[0].getElementsByTagName('a');
        for (var i=0; i<posts.length; i++)
            posts[i].setAttribute('target', '_blank');

    </script>
    </body>

</html>